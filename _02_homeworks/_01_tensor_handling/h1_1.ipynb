{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "a_tensor_initialization.py\n",
    "========="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a99e5bf278f654ec"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:35.816161100Z",
     "start_time": "2023-09-19T09:39:33.608618400Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor([1,2,3],device='cpu') # CPU에 텐서 생성 - 클래스의 객체로 생성\n",
    "print(t1.dtype) # t1 텐서의 데이터 타입\n",
    "print(t1.device) # t1 텐서의 디바이스\n",
    "print(t1.requires_grad) # t1 텐서의 requires_grad 속성 - default = False\n",
    "print(t1.size()) # t1 텐서의 크기\n",
    "print(t1.shape) # t1 텐서의 모양\n",
    "\n",
    "t1_cpu = t1.cpu() # t1 텐서를 cpu로 이동 -> 이미 cpu에 있기 때문에 변화없음"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:36.337154300Z",
     "start_time": "2023-09-19T09:39:36.321139600Z"
    }
   },
   "id": "5b88b5b46c7193c9"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int64\n",
      "cpu\n",
      "False\n",
      "torch.Size([3])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "t2 = torch.tensor([1,2,3],device='cpu') # CPU에 텐서 생성 - 함수를 호출하여 생성\n",
    "print(t2.dtype)  # >>> torch.int64\n",
    "print(t2.device)  # >>> cpu\n",
    "print(t2.requires_grad)  # >>> False\n",
    "print(t2.size())  # torch.Size([3])\n",
    "print(t2.shape)  # torch.Size([3])\n",
    "\n",
    "t2_cpu = t2.cpu() #t2 텐서를 cpu로 이동 -> 이미 cpu에 있기 때문에 변화없음"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:37.181779800Z",
     "start_time": "2023-09-19T09:39:37.166776500Z"
    }
   },
   "id": "1a2ae4849a6b88ec"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([]) 0\n",
      "tensor(1)\n"
     ]
    }
   ],
   "source": [
    "a1 = torch.tensor(1) # 텐서 객체 생성 -> 0차원 [1]\n",
    "print(a1.shape, a1.ndim) # shape -> 텐서의 모양 = [], ndim-> 텐서의 차원 = 0차원\n",
    "print(a1) # 텐서 내부 데이터 출력"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:37.771917400Z",
     "start_time": "2023-09-19T09:39:37.754913100Z"
    }
   },
   "id": "df0b5358d65a89de"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1]) 1\n",
      "tensor([1])\n"
     ]
    }
   ],
   "source": [
    "a2 = torch.tensor([1])\n",
    "print(a2.shape, a2.ndim)\n",
    "print(a2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:38.324065300Z",
     "start_time": "2023-09-19T09:39:38.316064200Z"
    }
   },
   "id": "6ab1981220fefab"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5]) 1\n",
      "tensor([1, 2, 3, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "a3 = torch.tensor([1, 2, 3, 4, 5])\n",
    "print(a3.shape, a3.ndim) # shape -> 1차원에 데이터 5개 = [5], ndim -> 1차원\n",
    "print(a3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:38.821176800Z",
     "start_time": "2023-09-19T09:39:38.809173800Z"
    }
   },
   "id": "9ecdcdabf651a80a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1]) 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3],\n",
      "        [4],\n",
      "        [5]])\n"
     ]
    }
   ],
   "source": [
    "a4 = torch.tensor([[1], [2], [3], [4], [5]])\n",
    "print(a4.shape, a4.ndim) # shape -> (1차원에 데이터 5개, 2차원에 데이터 1개) = [5,1], ndim -> 2차원\n",
    "print(a4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:39.373300Z",
     "start_time": "2023-09-19T09:39:39.358296400Z"
    }
   },
   "id": "d40364f05fe97ba5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2]) 2\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "a5 = torch.tensor([\n",
    "    [1, 2],\n",
    "    [3, 4],\n",
    "    [5, 6]\n",
    "])\n",
    "print(a5.shape, a5.ndim) # shape -> (1차원에 데이터 3개, 2차원에 데이터 2개) = [3,2], ndim -> 2차원\n",
    "print(a5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:39.927732700Z",
     "start_time": "2023-09-19T09:39:39.910728100Z"
    }
   },
   "id": "7640db9fca05348"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 1]) 3\n",
      "tensor([[[1],\n",
      "         [2]],\n",
      "\n",
      "        [[3],\n",
      "         [4]],\n",
      "\n",
      "        [[5],\n",
      "         [6]]])\n"
     ]
    }
   ],
   "source": [
    "a6 = torch.tensor([\n",
    "    [[1], [2]],\n",
    "    [[3], [4]],\n",
    "    [[5], [6]]\n",
    "])\n",
    "print(a6.shape, a6.ndim) # shape -> (1차원 : 3, 2차원 : 2, 3차원: 1) = [3,2,1], ndim -> 3\n",
    "print(a6)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:40.454849600Z",
     "start_time": "2023-09-19T09:39:40.436845800Z"
    }
   },
   "id": "a4cafb95f3de0906"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 1]) 4\n",
      "tensor([[[[1],\n",
      "          [2]]],\n",
      "\n",
      "\n",
      "        [[[3],\n",
      "          [4]]],\n",
      "\n",
      "\n",
      "        [[[5],\n",
      "          [6]]]])\n"
     ]
    }
   ],
   "source": [
    "a7 = torch.tensor([\n",
    "    [[[1], [2]]],\n",
    "    [[[3], [4]]],\n",
    "    [[[5], [6]]]\n",
    "])\n",
    "print(a7.shape, a7.ndim) # (1차원 : 3, 2차원 : 1, 3차원 : 2, 4차원 : 1) = [3,1,2,1], ndim -> 4 \n",
    "print(a7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:40.960966300Z",
     "start_time": "2023-09-19T09:39:40.947963900Z"
    }
   },
   "id": "b6c787e3a11d6bc9"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 3]) 4\n",
      "tensor([[[[1, 2, 3],\n",
      "          [2, 3, 4]]],\n",
      "\n",
      "\n",
      "        [[[3, 1, 1],\n",
      "          [4, 4, 5]]],\n",
      "\n",
      "\n",
      "        [[[5, 6, 2],\n",
      "          [6, 3, 1]]]])\n"
     ]
    }
   ],
   "source": [
    "a8 = torch.tensor([\n",
    "    [[[1, 2, 3], [2, 3, 4]]],\n",
    "    [[[3, 1, 1], [4, 4, 5]]],\n",
    "    [[[5, 6, 2], [6, 3, 1]]]\n",
    "])\n",
    "print(a8.shape, a8.ndim) # shape -> (1차원 : 3, 2차원 : 1, 3차원 : 2, 4차원 : 3) = [3,1,2,3], ndim -> 4\n",
    "print(a8)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:41.459425Z",
     "start_time": "2023-09-19T09:39:41.455424300Z"
    }
   },
   "id": "b6bbd0263ef76a19"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 1, 2, 3, 1]) 5\n",
      "tensor([[[[[1],\n",
      "           [2],\n",
      "           [3]],\n",
      "\n",
      "          [[2],\n",
      "           [3],\n",
      "           [4]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[3],\n",
      "           [1],\n",
      "           [1]],\n",
      "\n",
      "          [[4],\n",
      "           [4],\n",
      "           [5]]]],\n",
      "\n",
      "\n",
      "\n",
      "        [[[[5],\n",
      "           [6],\n",
      "           [2]],\n",
      "\n",
      "          [[6],\n",
      "           [3],\n",
      "           [1]]]]])\n"
     ]
    }
   ],
   "source": [
    "a9 = torch.tensor([\n",
    "    [[[[1], [2], [3]], [[2], [3], [4]]]],\n",
    "    [[[[3], [1], [1]], [[4], [4], [5]]]],\n",
    "    [[[[5], [6], [2]], [[6], [3], [1]]]]\n",
    "])\n",
    "print(a9.shape, a9.ndim) # shape -> (1차원 : 3, 2차원 : 1, 3차원 : 2, 4차원 : 3, 5차원 : 1) = [3,1,2,3,1], ndim -> 5\n",
    "print(a9)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:41.971124200Z",
     "start_time": "2023-09-19T09:39:41.955120500Z"
    }
   },
   "id": "3ee0d27cc5c0673c"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5]) 2\n",
      "tensor([[1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5],\n",
      "        [1, 2, 3, 4, 5]])\n"
     ]
    }
   ],
   "source": [
    "a10 = torch.tensor([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [1, 2, 3, 4, 5],\n",
    "])\n",
    "print(a10.shape, a10.ndim) # shape -> (1차원 : 4, 2차원 : 5) = [4,5], ndim -> 2\n",
    "print(a10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:42.455151800Z",
     "start_time": "2023-09-19T09:39:42.441149Z"
    }
   },
   "id": "f46bd1853650c586"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 5]) 3\n",
      "tensor([[[1, 2, 3, 4, 5]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5]],\n",
      "\n",
      "        [[1, 2, 3, 4, 5]]])\n"
     ]
    }
   ],
   "source": [
    "a10 = torch.tensor([\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "    [[1, 2, 3, 4, 5]],\n",
    "])\n",
    "print(a10.shape, a10.ndim) # shape -> (1차원 : 4, 2차원 : 1, 3차원 : 5) = [4,1,5], ndim = 3\n",
    "print(a10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:42.916774900Z",
     "start_time": "2023-09-19T09:39:42.898770400Z"
    }
   },
   "id": "f8e5d2037c72a6d8"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "expected sequence of length 3 at dim 3 (got 2)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[16], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m a11 \u001B[38;5;241m=\u001B[39m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43m                 \u001B[49m\u001B[38;5;66;43;03m# ValueError: expected sequence of length 3 at dim 3 (got 2)\u001B[39;49;00m\n\u001B[0;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m3\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m4\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m5\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m      6\u001B[0m \u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(a11\u001B[38;5;241m.\u001B[39mshape,a11\u001B[38;5;241m.\u001B[39mndim)\n",
      "\u001B[1;31mValueError\u001B[0m: expected sequence of length 3 at dim 3 (got 2)"
     ]
    }
   ],
   "source": [
    "a11 = torch.tensor([                 # ValueError: expected sequence of length 3 at dim 3 (got 2)\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "    [[[1, 2, 3], [4, 5]]],\n",
    "])\n",
    "print(a11.shape,a11.ndim)\n",
    "#텐서객체 생성 안됨 -> 텐서는 각 차원의 길이가 모두 동일해야 하는데 4차원의 길이가 3인것과 2인것이 존재하기 때문에 텐서를 생성할 수 없다."
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:44.024032700Z",
     "start_time": "2023-09-19T09:39:43.815986100Z"
    }
   },
   "id": "6eadf7ca13ed79e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "b_tensor_initialization_copy.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "93ad1c54044bddf4"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3])\n",
      "tensor([1, 2, 3])\n",
      "####################################################################################################\n",
      "tensor([1., 2., 3.])\n",
      "tensor([1, 2, 3], dtype=torch.int32)\n",
      "tensor([100,   2,   3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "l1 = [1, 2, 3]\n",
    "t1 = torch.Tensor(l1)\n",
    "\n",
    "l2 = [1, 2, 3]\n",
    "t2 = torch.tensor(l2)\n",
    "\n",
    "l3 = [1, 2, 3]\n",
    "t3 = torch.as_tensor(l3)\n",
    "\n",
    "l1[0] = 100\n",
    "l2[0] = 100\n",
    "l3[0] = 100\n",
    "\n",
    "print(t1)\n",
    "print(t2)\n",
    "print(t3)\n",
    "# 위 코드의 출력 결과를 통해 파이썬 리스트를 이용해 텐서객체를 생성하면 깊은복사가 되는것을 알 수 있다.\n",
    "print(\"#\" * 100)\n",
    "\n",
    "l4 = np.array([1, 2, 3])\n",
    "t4 = torch.Tensor(l4)\n",
    "\n",
    "l5 = np.array([1, 2, 3])\n",
    "t5 = torch.tensor(l5)\n",
    "\n",
    "l6 = np.array([1, 2, 3])\n",
    "t6 = torch.as_tensor(l6)\n",
    "\n",
    "l4[0] = 100\n",
    "l5[0] = 100\n",
    "l6[0] = 100\n",
    "\n",
    "print(t4)\n",
    "print(t5)\n",
    "print(t6)\n",
    "# 위 코드의 출력 결과를 통해 numpy를 이용해 텐서객체를 생성하면 as_tensor함수는 얕은복사로인해 원본의 수정을 반영하고\n",
    "# Tensor 또는 tensor를 이용해 생성하면 깊은복사로 인해 원본의 수정을 반영하지 않는다.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:46.746524900Z",
     "start_time": "2023-09-19T09:39:46.730521200Z"
    }
   },
   "id": "10824b0ac5ba027"
  },
  {
   "cell_type": "markdown",
   "source": [
    "c_tensor_initialization_constant_values.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abd24126c0f03412"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "tensor([ 1.4013e-45,  0.0000e+00, -2.2015e+18,  5.2128e-43])\n",
      "tensor([ 1.4013e-45,  0.0000e+00, -2.2468e+18,  5.2128e-43])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.ones(size=(5,))  # 0차원에 원소 5개를 1로 가진 텐서 생성\n",
    "t1_like = torch.ones_like(input=t1) # t1과 사이즈가 동일한 크기와 데이터타입을 가진 텐서 객체를 생성하고 모든 원소를 1로 만듬\n",
    "print(t1)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "print(t1_like)  # >>> tensor([1., 1., 1., 1., 1.])\n",
    "\n",
    "t2 = torch.zeros(size=(6,))  # 0차원에 원소 6개를 0으로 가진 텐서 생성\n",
    "t2_like = torch.zeros_like(input=t2) # t2와 사이즈가 동일한 크기와 데이터타입을 가진 텐서 객체를 생성하고 모든 원소를 0로 만듬\n",
    "print(t2)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "print(t2_like)  # >>> tensor([0., 0., 0., 0., 0., 0.])\n",
    "\n",
    "t3 = torch.empty(size=(4,))  # 0차원에 원소 6개를 초기화되지 않은 값(무작위)으로 가진 텐서 생성\n",
    "t3_like = torch.empty_like(input=t3) # t3와 사이즈가 동일한 크기와 데이터타입을 가진 텐서 객체를 생성하고 모든 원소를 초기화되지 않은 값(무작위)으로 만듬\n",
    "print(t3)  # >>> tensor([0., 0., 0., 0.])\n",
    "print(t3_like)  # >>> tensor([0., 0., 0., 0.])\n",
    "\n",
    "t4 = torch.eye(n=3) # 항등행렬 생성\n",
    "print(t4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:47.820745700Z",
     "start_time": "2023-09-19T09:39:47.797740700Z"
    }
   },
   "id": "723368371e9d4e2c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "d_tensor_initialization_random_values.py\n",
    "==="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8d7795cf70d19d4"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[14, 17]])\n",
      "tensor([[0.5901, 0.6124, 0.6159]])\n",
      "tensor([[ 0.4781, -1.2679, -0.9556]])\n",
      "tensor([[ 9.9052,  9.2344],\n",
      "        [10.0432, 11.3663],\n",
      "        [ 9.1996, 10.3871]])\n",
      "tensor([0.0000, 2.5000, 5.0000])\n",
      "tensor([0, 1, 2, 3, 4])\n",
      "##############################\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n",
      "\n",
      "tensor([[0.3126, 0.3791, 0.3087],\n",
      "        [0.0736, 0.4216, 0.0691]])\n",
      "tensor([[0.2332, 0.4047, 0.2162],\n",
      "        [0.9927, 0.4128, 0.5938]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.randint(low=10, high=20, size=(1,2)) #10 이상 20 미만의 랜덤 정수를 1차원 원소 2개에 넣는다.\n",
    "print(t1)\n",
    "\n",
    "t2 = torch.rand(size=(1, 3)) #1이상 3미만의 랜덤 실수를 1차원 원소 3개에 넣는다.\n",
    "print(t2)\n",
    "\n",
    "t3 = torch.randn(size=(1, 3)) #1이상 3미만의 정규분포 내의 확률의 랜덤 실수를 1차원 원소 3개에 넣는다\n",
    "print(t3)\n",
    "\n",
    "t4 = torch.normal(mean=10.0, std=1.0, size=(3, 2)) #평균 10, 표준편차 1을 가진 정규분포 내의 확률의 랜덤 실수를 3x2 텐서에 넣는다.\n",
    "print(t4)\n",
    "\n",
    "t5 = torch.linspace(start=0.0, end=5.0, steps=3) #0부터 5까지 간격이 같은 원소3개를 1차원에 넣는다\n",
    "print(t5)\n",
    "\n",
    "t6 = torch.arange(5) #0부터 4까지 정수를 차례대로 1차원에 넣는다\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random1 = torch.rand(2, 3)\n",
    "print(random1)\n",
    "\n",
    "random2 = torch.rand(2, 3)\n",
    "print(random2)\n",
    "\n",
    "print()\n",
    "\n",
    "torch.manual_seed(1729)\n",
    "random3 = torch.rand(2, 3)\n",
    "print(random3)\n",
    "\n",
    "random4 = torch.rand(2, 3)\n",
    "print(random4)\n",
    "\n",
    "# seed를 설정한 시점 기준으로 rand함수는 같은 호출횟수일 때 같은 값을 출력한다.\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:48.773959600Z",
     "start_time": "2023-09-19T09:39:48.760957300Z"
    }
   },
   "id": "b1f8e2dcc8c082f8"
  },
  {
   "cell_type": "markdown",
   "source": [
    "e_tensor_type_conversion.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b38156cbf0e9008b"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int16)\n",
      "tensor([[18.0429,  7.2532, 19.6519],\n",
      "        [10.8626,  2.1505, 19.6913]], dtype=torch.float64)\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]], dtype=torch.int32)\n",
      "torch.float32\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float64\n",
      "torch.int16\n",
      "torch.float16\n",
      "torch.int16\n",
      "torch.float64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.ones((2, 3)) # ones함수는 float32의 데이터타입을 가진다\n",
    "print(a.dtype)\n",
    "\n",
    "b = torch.ones((2, 3), dtype=torch.int16) # 데이터타입 int16으로 2x3텐서에 1을 채워넣는다\n",
    "print(b)\n",
    "\n",
    "c = torch.rand((2, 3), dtype=torch.float64) * 20. # 데이터타입 float64로 2x3텐서에 1을 채워넣고 각원소에 20 곱하기\n",
    "print(c)\n",
    "\n",
    "d = b.to(torch.int32) # 텐서 b의 데이터타입을 int32로 변경\n",
    "print(d)\n",
    "\n",
    "double_d = torch.ones(10, 2, dtype=torch.float32) # 데이터타입 double로 10x2 텐서에 1채워넣기\n",
    "short_e = torch.tensor([[1, 2]], dtype=torch.short) # 데이터타입 short로 1x2 텐서의 원소를 변경\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "double_d = torch.zeros(10, 2).double() # 데이터타입 double로 10x2 텐서에 0채워넣기\n",
    "short_e = torch.ones(10, 2).short() # 데이터타입 short로 10x2 텐서에 1채워넣기\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "double_g = torch.zeros(10, 2).to(torch.double) # 데이터타입 double로 10x2 텐서에 0채워넣기\n",
    "short_e = torch.ones(10, 2).to(dtype=torch.short) #데이터타입 short 10x2 텐서에 1채워넣기\n",
    "\n",
    "print(double_g.dtype)\n",
    "print(short_e.dtype)\n",
    "double_d = torch.zeros(10, 2).type(torch.float16) #데이터타입 double로 10x2텐서에 0채워넣기\n",
    "short_e = torch.ones(10, 2). type(dtype=torch.short) #데이터타입 short 10x2 텐서에 1채워넣기\n",
    "\n",
    "print(double_d.dtype)\n",
    "print(short_e.dtype)\n",
    "\n",
    "\"\"\"\n",
    "형 변환 방법 정리\n",
    "1. (tensor).to(dtype)\n",
    "2. (tensor).(dtype)\n",
    "3. (tensor).type(dtype)\n",
    "4. torch.tensor(list,dtype=(dtype))\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "double_f = torch.rand(5, dtype=torch.double) #랜덤\n",
    "short_g = double_f.to(torch.short)\n",
    "print((double_f * short_g).dtype) # double * short = double\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:49.741176100Z",
     "start_time": "2023-09-19T09:39:49.711170Z"
    }
   },
   "id": "c69330cd70962b96"
  },
  {
   "cell_type": "markdown",
   "source": [
    "f_tensor_operations.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7cb88c5344ff5cc7"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "tensor([[2., 2., 2.],\n",
      "        [2., 2., 2.]])\n",
      "##############################\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "##############################\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#두 텐서의 덧셈\n",
    "t1 = torch.ones(size=(2, 3)) # 2x3 텐서에 0 채우기\n",
    "t2 = torch.ones(size=(2, 3)) # 2x3 텐서에 0 채우기\n",
    "t3 = torch.add(t1, t2) # 두 텐서 더하기\n",
    "t4 = t1 + t2 # 두 텐서 더하기\n",
    "print(t3)\n",
    "print(t4)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "#두 텐서의 뺄셈\n",
    "t5 = torch.sub(t1, t2)\n",
    "t6 = t1 - t2\n",
    "print(t5)\n",
    "print(t6)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "#두 텐서의 곱셈\n",
    "t7 = torch.mul(t1, t2)\n",
    "t8 = t1 * t2\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 30)\n",
    "\n",
    "#두 텐서의 나눗셈\n",
    "t9 = torch.div(t1, t2)\n",
    "t10 = t1 / t2\n",
    "print(t9)\n",
    "print(t10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:50.675056700Z",
     "start_time": "2023-09-19T09:39:50.661053Z"
    }
   },
   "id": "4b96b724eb6ea634"
  },
  {
   "cell_type": "markdown",
   "source": [
    "g_tensor_operations_mm.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b4f2d89c52f410e5"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7) torch.Size([])\n",
      "tensor([[1.6750, 2.2840],\n",
      "        [0.0956, 1.0294]]) torch.Size([2, 2])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.dot(\n",
    "  torch.tensor([2, 3]), torch.tensor([2, 1]) # tensor.dot = 벡터 내적\n",
    ")\n",
    "print(t1, t1.size())\n",
    "\n",
    "t2 = torch.randn(2, 3)\n",
    "t3 = torch.randn(3, 2)\n",
    "t4 = torch.mm(t2, t3) # tensor.mm = 행렬 곱셈\n",
    "print(t4, t4.size())\n",
    "\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(10, 4, 5)\n",
    "t7 = torch.bmm(t5, t6) # tensor.bmm = 배치행렬 곱셈\n",
    "# 배치행렬이란 여러개의 2D행렬을 하나의 텐서로 묶어서 표현한 데이터 구조\n",
    "print(t7.size())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:51.694291400Z",
     "start_time": "2023-09-19T09:39:51.639278600Z"
    }
   },
   "id": "3da079fcaa3d60b7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "h_tensor_operations_matmul.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cb38ff4fc824573a"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor(-3.9578)\n",
      "torch.Size([3])\n",
      "torch.Size([10, 3])\n",
      "torch.Size([10, 3, 5])\n",
      "torch.Size([10, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# vector x vector: dot product\n",
    "t1 = torch.randn(3)\n",
    "t2 = torch.randn(3)\n",
    "print(torch.matmul(t1, t2).size())  # torch.Size([])\n",
    "print(torch.matmul(t1, t2)) # torch.matmul = 행렬 곱셈 함수 -> 다양한 텐서 형태와 브로드캐스팅 지원\n",
    "# matrix x vector: broadcasted dot\n",
    "t3 = torch.randn(3, 4)\n",
    "t4 = torch.randn(4)\n",
    "print(torch.matmul(t3, t4).size())  # torch.Size([3])\n",
    "\n",
    "# batched matrix x vector: broadcasted dot\n",
    "t5 = torch.randn(10, 3, 4)\n",
    "t6 = torch.randn(4)\n",
    "print(torch.matmul(t5, t6).size())  # torch.Size([10, 3])\n",
    "\n",
    "# batched matrix x batched matrix: bmm\n",
    "t7 = torch.randn(10, 3, 4)\n",
    "t8 = torch.randn(10, 4, 5)\n",
    "print(torch.matmul(t7, t8).size())  # torch.Size([10, 3, 5])\n",
    "#1차원 원소 각 10개 내의 3x4 행렬과 4x5행렬이 곱해져 3x5행렬이 나옴\n",
    "#결과적으로 [10,3,5]의 형태가 됨\n",
    "# batched matrix x matrix: bmm\n",
    "t9 = torch.randn(10, 3, 4)\n",
    "t10 = torch.randn(4, 5)\n",
    "print(torch.matmul(t9, t10).size())  # torch.Size([10, 3, 5])\n",
    "#t9에 t10이 곱해질 때 broadcats에 의해 t9의 1차원 원소 10개 각각에 t10 텐서가 곱해져\n",
    "#3x4 행렬과 4x5행렬이 곱해져 [10,3,5]의 형태가 됨"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:52.654504300Z",
     "start_time": "2023-09-19T09:39:52.603493200Z"
    }
   },
   "id": "80456bc2f7762507"
  },
  {
   "cell_type": "markdown",
   "source": [
    "i_tensor_broadcasting.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "298a1951f16f0c19"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2., 4., 6.])\n",
      "################################################## 1\n",
      "tensor([[-4, -4],\n",
      "        [-2, -1],\n",
      "        [ 6,  5]])\n",
      "################################################## 2\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]])\n",
      "tensor([[-1.,  0.],\n",
      "        [ 1.,  2.]])\n",
      "tensor([[2., 4.],\n",
      "        [6., 8.]])\n",
      "tensor([[0.5000, 1.0000],\n",
      "        [1.5000, 2.0000]])\n",
      "################################################## 3\n",
      "torch.Size([3, 28, 28])\n",
      "################################################## 4\n",
      "tensor([[4, 3],\n",
      "        [3, 4]])\n",
      "tensor([[6, 7],\n",
      "        [2, 5]])\n",
      "tensor([[8, 6],\n",
      "        [5, 3]])\n",
      "tensor([[ 8,  9],\n",
      "        [ 7, 10]])\n",
      "################################################## 5\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([4, 3, 2])\n",
      "torch.Size([5, 3, 4, 1])\n",
      "################################################## 6\n",
      "torch.Size([5, 3, 4, 1])\n",
      "torch.Size([3, 1, 7])\n",
      "torch.Size([3, 3, 3])\n",
      "################################################## 7\n",
      "tensor([5., 5., 5., 5.])\n",
      "tensor([25., 25., 25., 25.])\n",
      "tensor([  1.,   4.,  27., 256.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1.0, 2.0, 3.0])\n",
    "t2 = 2.0\n",
    "print(t1 * t2)\n",
    "# broadcasting으로 t1원소 각각에 t2원소가 곱해짐\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t3 = torch.tensor([[0, 1], [2, 4], [10, 10]])\n",
    "t4 = torch.tensor([4, 5])\n",
    "print(t3 - t4)\n",
    "# broadcasting으로 t3의 1차원 원소 각각에 t4원소가 빼짐\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t5 = torch.tensor([[1., 2.], [3., 4.]])\n",
    "print(t5 + 2.0)  # t5.add(2.0)\n",
    "print(t5 - 2.0)  # t5.sub(2.0)\n",
    "print(t5 * 2.0)  # t5.mul(2.0)\n",
    "print(t5 / 2.0)  # t5.div(2.0)\n",
    "# broadcasting으로 덧셈, 뺄셈, 곱셈, 나눗셈 연산이 가능\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "\n",
    "def normalize(x):\n",
    "  return x / 255\n",
    "\n",
    "\n",
    "t6 = torch.randn(3, 28, 28)\n",
    "print(normalize(t6).size())\n",
    "#함수를 이용한 broadcasting\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t7 = torch.tensor([[1, 2], [0, 3]])  # torch.Size([2, 2])\n",
    "t8 = torch.tensor([[3, 1]])  # torch.Size([1, 2])\n",
    "t9 = torch.tensor([[5], [2]])  # torch.Size([2, 1])\n",
    "t10 = torch.tensor([7])  # torch.Size([1])\n",
    "print(t7 + t8)   # >>> tensor([[4, 3], [3, 4]])\n",
    "print(t7 + t9)   # >>> tensor([[6, 7], [2, 5]])\n",
    "print(t8 + t9)   # >>> tensor([[8, 6], [5, 3]])\n",
    "print(t7 + t10)  # >>> tensor([[ 8, 9], [ 7, 10]])\n",
    "\n",
    "print(\"#\" * 50, 5)\n",
    "\n",
    "t11 = torch.ones(4, 3, 2)\n",
    "t12 = t11 * torch.rand(3, 2)  # 3rd & 2nd dims identical to t11, dim 0 absent\n",
    "print(t12.shape)\n",
    "#t11의 3,2에 t12가 곱해짐\n",
    "t13 = torch.ones(4, 3, 2)\n",
    "t14 = t13 * torch.rand(3, 1)  # 3rd dim = 1, 2nd dim is identical to t13\n",
    "print(t14.shape)\n",
    "#t13의 3,2에 t14의 3,1이 브로드캐스팅되어 3,2로 바뀐 뒤 곱해짐\n",
    "t15 = torch.ones(4, 3, 2)\n",
    "t16 = t15 * torch.rand(1, 2)  # 3rd dim is identical to t15, 2nd dim is 1\n",
    "print(t16.shape)\n",
    "#t15의 3,2에 t16의 1,2이 브로드캐스팅되어 3,2로 바뀐 뒤 곱해짐\n",
    "t17 = torch.ones(5, 3, 4, 1)\n",
    "t18 = torch.rand(3, 1, 1)  # 2nd dim is identical to t17, 3rd and 4th dims are 1\n",
    "print((t17 + t18).size())\n",
    "#t13의 3,4,1에 t14의 3,1,1이 브로드캐스팅되어 3,4,1로 바뀐 뒤 곱해짐\n",
    "print(\"#\" * 50, 6)\n",
    "\n",
    "t19 = torch.empty(5, 1, 4, 1)\n",
    "t20 = torch.empty(3, 1, 1)\n",
    "print((t19 + t20).size())  # torch.Size([5, 3, 4, 1])\n",
    "#t20의 1,1이 브로드캐스팅에 의해 4,1이되고 t19의 1,4,1이 브로드캐스팅에 의해 3,4,1이 되어 더해진다\n",
    "t21 = torch.empty(1)\n",
    "t22 = torch.empty(3, 1, 7)\n",
    "print((t21 + t22).size())  # torch.Size([3, 1, 7])\n",
    "#t22에 의해 t21이 3,1,7이되어 더해짐\n",
    "t23 = torch.ones(3, 3, 3)\n",
    "t24 = torch.ones(3, 1, 3)\n",
    "print((t23 + t24).size())  # torch.Size([3, 3, 3])\n",
    "#t24의 1,3이 브로드캐스팅에 의해 3,3이 되어 더해짐\n",
    "# t25 = torch.empty(5, 2, 4, 1)\n",
    "# t26 = torch.empty(3, 1, 1)\n",
    "# print((t25 + t26).size())\n",
    "# RuntimeError: The size of tensor a (2) must match\n",
    "# the size of tensor b (3) at non-singleton dimension 1\n",
    "\n",
    "print(\"#\" * 50, 7)\n",
    "\n",
    "t27 = torch.ones(4) * 5\n",
    "print(t27)  # >>> tensor([ 5, 5, 5, 5])\n",
    "\n",
    "t28 = torch.pow(t27, 2)\n",
    "print(t28)  # >>> tensor([ 25, 25, 25, 25])\n",
    "\n",
    "exp = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "a = torch.arange(1., 5.)  # tensor([ 1.,  2.,  3.,  4.])\n",
    "t29 = torch.pow(a, exp)\n",
    "print(t29)  # >>> tensor([   1.,    4.,   27.,  256.])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:53.597044100Z",
     "start_time": "2023-09-19T09:39:53.544032100Z"
    }
   },
   "id": "e724182f5bf149cd"
  },
  {
   "cell_type": "markdown",
   "source": [
    "j_tensor_indexing_slicing.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38b294d973ee6da5"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 6, 7, 8, 9])\n",
      "tensor([ 1,  6, 11])\n",
      "tensor(7)\n",
      "tensor([ 4,  9, 14])\n",
      "################################################## 1\n",
      "tensor([[ 5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14]])\n",
      "tensor([[ 8,  9],\n",
      "        [13, 14]])\n",
      "################################################## 2\n",
      "tensor([[0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.]])\n",
      "################################################## 3\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 3, 4, 5]])\n",
      "tensor([[3, 4],\n",
      "        [6, 7]])\n",
      "tensor([[2, 3, 4],\n",
      "        [3, 4, 5],\n",
      "        [6, 7, 8]])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [2, 0, 0, 5],\n",
      "        [5, 0, 0, 8]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(\n",
    "    [[0, 1, 2, 3, 4],\n",
    "     [5, 6, 7, 8, 9],\n",
    "     [10, 11, 12, 13, 14]]\n",
    ")\n",
    "\n",
    "print(x[1])  # >>> tensor([5, 6, 7, 8, 9]) # 1행 인덱싱\n",
    "print(x[:, 1])  # >>> tensor([1, 6, 11]) # 모든 행의 1열 인덱싱\n",
    "print(x[1, 2])  # >>> tensor(7) # 1행 2열 인덱싱\n",
    "print(x[:, -1])  # >>> tensor([4, 9, 14) # 모든 행의 마지막 열 인덱싱\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "print(x[1:])  # >>> tensor([[ 5,  6,  7,  8,  9], [10, 11, 12, 13, 14]]) # 1행부터 끝까지 인덱싱\n",
    "print(x[1:, 3:])  # >>> tensor([[ 8,  9], [13, 14]]) #1행부터 끝까지 3열부터 끝까지 인덱싱\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "y = torch.zeros((6, 6)) # 6x6 텐서 0 채우기\n",
    "y[1:4, 2] = 1 # 1행부터 3행까지 2열의 값을 1로 초기화\n",
    "print(y)\n",
    "\n",
    "print(y[1:4, 1:4]) # 1행부터 3행까지 1열부터 3열까지 출력\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "z = torch.tensor(\n",
    "    [[1, 2, 3, 4],\n",
    "     [2, 3, 4, 5],\n",
    "     [5, 6, 7, 8]]\n",
    ") # 3x4 텐서\n",
    "print(z[:2]) #0부터 1행까지 출력\n",
    "print(z[1:, 1:3]) #1부터 끝까지, 1열부터 2열까지 출력\n",
    "print(z[:, 1:]) #모든행, 1열부터 끝까지 출력\n",
    "\n",
    "z[1:, 1:3] = 0 #1행부터 모든행, 1열부터 2열가지 값을 0으로 초기화\n",
    "print(z)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:54.523572100Z",
     "start_time": "2023-09-19T09:39:54.508568800Z"
    }
   },
   "id": "6e50b3c685b2f630"
  },
  {
   "cell_type": "markdown",
   "source": [
    "k_tensor_reshaping.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f1d9ec58ef12611a"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t2 tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t3 tensor([[1, 2, 3, 4, 5, 6]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3])\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "################################################## 2\n",
      "tensor([[1],\n",
      "        [2],\n",
      "        [3]])\n",
      "tensor([[[1, 2, 3]],\n",
      "\n",
      "        [[4, 5, 6]]]) torch.Size([2, 1, 3])\n",
      "################################################## 3\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([1, 2, 3, 4, 5, 6, 7, 8])\n",
      "t17 tensor([[1, 2, 3, 4],\n",
      "        [5, 6, 7, 8]])\n",
      "################################################## 4\n",
      "tensor([[[-0.0303,  0.0763, -0.1469, -0.0251, -1.2086],\n",
      "         [ 0.6541, -0.1748, -0.6700,  0.7820,  0.1549],\n",
      "         [-0.8908,  0.1875, -0.2166,  0.7557, -1.2399]],\n",
      "\n",
      "        [[ 0.3925,  0.7325, -0.5350, -0.3018,  1.1425],\n",
      "         [ 0.6591, -0.9338,  0.0862, -0.2476,  0.4155],\n",
      "         [-0.4618, -0.5610, -0.5790, -0.8500, -0.1736]]])\n",
      "tensor([[[-0.0303,  0.6541, -0.8908],\n",
      "         [ 0.3925,  0.6591, -0.4618]],\n",
      "\n",
      "        [[ 0.0763, -0.1748,  0.1875],\n",
      "         [ 0.7325, -0.9338, -0.5610]],\n",
      "\n",
      "        [[-0.1469, -0.6700, -0.2166],\n",
      "         [-0.5350,  0.0862, -0.5790]],\n",
      "\n",
      "        [[-0.0251,  0.7820,  0.7557],\n",
      "         [-0.3018, -0.2476, -0.8500]],\n",
      "\n",
      "        [[-1.2086,  0.1549, -1.2399],\n",
      "         [ 1.1425,  0.4155, -0.1736]]])\n",
      "t18_1 False\n",
      "t20 :  tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "t20 is contiguous :  True\n",
      "t21 :  tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "t21 is contiguous :  False\n",
      "t22 :  tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "t22 is contiguous :  False\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) #2x3 텐서\n",
    "t2 = t1.view(3, 2)  # Shape becomes (3, 2) # \n",
    "t3 = t1.reshape(1, 6)  # Shape becomes (1, 6)\n",
    "print('t2',t2)\n",
    "print(t1)\n",
    "print('t3',t3)\n",
    "print(t1)\n",
    "\n",
    "\"\"\"\n",
    "view와 reshape의 차이\n",
    "\n",
    "두 함수의 사용 원리와 반환 결과가 동일해보이지만 차이가 있다.\n",
    "바로 contiguous 속성을 만족하지 않은 텐서에 적용이 가능하냐 여부이다.\n",
    "\n",
    "contiguous란 메모리 내에서 자료형을 저장하는 상태에 대한 것이다.\n",
    "텐서의 자료의 저장 순서가 순서대로라면 contiguous가 True이며\n",
    "순서가 원래방향과 어긋나있다면 contigugous가 False이다.\n",
    "\n",
    "view는 contiguous가 False인 텐서를 사용할 수 없으며\n",
    "reshape는 사용 가능하다.\n",
    "\"\"\"\n",
    "\n",
    "t4 = torch.arange(8).view(2, 4)  # Shape becomes (2, 4) #1, -> 2x4텐서\n",
    "t5 = torch.arange(6).view(2, 3)  # Shape becomes (2, 3) #1, -> 2x3텐서\n",
    "print(t4)\n",
    "print(t5)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "# Original tensor with shape (1, 3, 1)\n",
    "t6 = torch.tensor([[[1], [2], [3]]]) #1x3x1텐서 생성\n",
    "\n",
    "# Remove all dimensions of size 1\n",
    "t7 = t6.squeeze()  # Shape becomes (3,) # 1인 차원 삭제 1,3,1 -> 3,\n",
    "\n",
    "# Remove dimension at position 0\n",
    "t8 = t6.squeeze(0)  # Shape becomes (3, 1) #1차원이 1이면 삭제 1,3,1 -> 3,1\n",
    "print(t7)\n",
    "print(t8)\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "# Original tensor with shape (3,)\n",
    "t9 = torch.tensor([1, 2, 3]) #3, 인 텐서 생성\n",
    "\n",
    "# Add a new dimension at position 1\n",
    "t10 = t9.unsqueeze(1)  # Shape becomes (3, 1) # 1차원에 크기가 1인 차원 생성 3, -> 3,1\n",
    "print(t10)\n",
    "\n",
    "t11 = torch.tensor(\n",
    "  [[1, 2, 3],\n",
    "   [4, 5, 6]]\n",
    ")\n",
    "t12 = t11.unsqueeze(1)  # Shape becomes (2, 1, 3) # 1차원에 크기가 1인 차원 생성 2,3-> 2,1,3\n",
    "print(t12, t12.shape)\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t13 = torch.tensor([[1, 2, 3], [4, 5, 6]]) #2,3 텐서 생성\n",
    "\n",
    "# Flatten the tensor\n",
    "t14 = t13.flatten()  # Shape becomes (6,) # 1차원으로 쭉 펴기 2,3 -> 6,\n",
    "\n",
    "print(t14)\n",
    "\n",
    "# Original tensor with shape (2, 2, 2)\n",
    "t15 = torch.tensor([[[1, 2],\n",
    "                     [3, 4]],\n",
    "                    [[5, 6],\n",
    "                     [7, 8]]]) # 2,2,2 텐서 생성\n",
    "t16 = torch.flatten(t15) # 1차원으로 펴기\n",
    "\n",
    "\n",
    "t17 = torch.flatten(t15, start_dim=1) # 2차원부터 쭉 펴기\n",
    "\n",
    "print(t16)\n",
    "print('t17',t17)\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t18 = torch.randn(2, 3, 5) # 2,3,5에 정규분포 확률의 랜덤 실수 생성\n",
    "print(t18)  # >>> torch.Size([2, 3, 5])\n",
    "print(torch.permute(t18, (2, 0, 1)))  # >>> torch.Size([5, 2, 3]) #차원의 구조 변경 2,3,5 -> 5,2,3\n",
    "t18_1 = torch.permute(t18, (2, 0, 1))\n",
    "print('t18_1',t18_1.is_contiguous()) # permute 사용 시 contiguous성질이 사라진다\n",
    "\n",
    "# Original tensor with shape (2, 3)\n",
    "t19 = torch.tensor([[1, 2, 3], [4, 5, 6]]) # 2,3 텐서 생성\n",
    "\n",
    "\n",
    "\n",
    "# Permute the dimensions\n",
    "t20 = torch.permute(t19, dims=(0, 1))  # Shape becomes (2, 3) still\n",
    "print('t20 : ',t20)\n",
    "print('t20 is contiguous : ',t20.is_contiguous()) # permute 시 shape의 변화가 없다면 contiguous 성질 유지\n",
    "\n",
    "t21 = torch.permute(t19, dims=(1, 0))  # Shape becomes (3, 2)\n",
    "print('t21 : ',t21)\n",
    "print('t21 is contiguous : ',t21.is_contiguous()) # permute 시 shape의 변화가 생혀 contiguous 성질이 사라짐\n",
    "\n",
    "\n",
    "# Transpose the tensor\n",
    "t22 = torch.transpose(t19, 0, 1)  # Shape becomes (3, 2)\n",
    "print('t22 : ',t22)\n",
    "print('t22 is contiguous : ',t22.is_contiguous()) # transpose도 마찬가지 contiguous 성질이 사라진다\n",
    "\n",
    "t23 = torch.t(t19)  # Shape becomes (3, 2) # 전치 함수\n",
    "\n",
    "print(t23)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:55.496576900Z",
     "start_time": "2023-09-19T09:39:55.462569700Z"
    }
   },
   "id": "3df8d8c60854a97f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "l_tensor_concat.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d356c943a12c5b6a"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 6, 3])\n",
      "################################################## 1\n",
      "torch.Size([8])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "################################################## 2\n",
      "torch.Size([4, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11]])\n",
      "torch.Size([2, 6])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8],\n",
      "        [ 3,  4,  5,  9, 10, 11]])\n",
      "################################################## 3\n",
      "torch.Size([6, 3])\n",
      "tensor([[ 0,  1,  2],\n",
      "        [ 3,  4,  5],\n",
      "        [ 6,  7,  8],\n",
      "        [ 9, 10, 11],\n",
      "        [12, 13, 14],\n",
      "        [15, 16, 17]])\n",
      "torch.Size([2, 9])\n",
      "tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
      "        [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
      "################################################## 4\n",
      "torch.Size([2, 2, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5]],\n",
      "\n",
      "        [[ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 4, 3])\n",
      "tensor([[[ 0,  1,  2],\n",
      "         [ 3,  4,  5],\n",
      "         [ 6,  7,  8],\n",
      "         [ 9, 10, 11]]])\n",
      "torch.Size([1, 2, 6])\n",
      "tensor([[[ 0,  1,  2,  6,  7,  8],\n",
      "         [ 3,  4,  5,  9, 10, 11]]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.zeros([2, 1, 3])\n",
    "t2 = torch.zeros([2, 3, 3])\n",
    "t3 = torch.zeros([2, 2, 3])\n",
    "\n",
    "t4 = torch.cat([t1, t2, t3], dim=1) #1차원 결합 -> 2,6,3\n",
    "print(t4.shape)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t5 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t6 = torch.arange(3, 8)  # tensor([3, 4, 5, 6, 7])\n",
    "\n",
    "t7 = torch.cat((t5, t6), dim=0) # 0차원 결합 -> 8,\n",
    "print(t7.shape)  # >>> torch.Size([8])\n",
    "print(t7)  # >>> tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "print(\"#\" * 50, 2)\n",
    "\n",
    "t8 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t9 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "# 2차원 텐서간 병합\n",
    "t10 = torch.cat((t8, t9), dim=0) #2차원 텐서의 0차원 결합 -> 4,3\n",
    "print(t10.size())  # >>> torch.Size([4, 3])\n",
    "print(t10)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11]])\n",
    "\n",
    "t11 = torch.cat((t8, t9), dim=1) #2차원 텐서의 1차원 결합 ->2,6\n",
    "print(t11.size())  # >>>torch.Size([2, 6])\n",
    "print(t11)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8],\n",
    "#             [ 3,  4,  5,  9, 10, 11]])\n",
    "\n",
    "print(\"#\" * 50, 3)\n",
    "\n",
    "t12 = torch.arange(0, 6).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t13 = torch.arange(6, 12).reshape(2, 3)  # torch.Size([2, 3])\n",
    "t14 = torch.arange(12, 18).reshape(2, 3)  # torch.Size([2, 3])\n",
    "\n",
    "t15 = torch.cat((t12, t13, t14), dim=0) #2차원 텐서의 차원의 차원 0 결합 -> 6,3\n",
    "print(t15.size())  # >>> torch.Size([6, 3])\n",
    "print(t15)\n",
    "# >>> tensor([[ 0,  1,  2],\n",
    "#             [ 3,  4,  5],\n",
    "#             [ 6,  7,  8],\n",
    "#             [ 9, 10, 11],\n",
    "#             [12, 13, 14],\n",
    "#             [15, 16, 17]])\n",
    "\n",
    "t16 = torch.cat((t12, t13, t14), dim=1) #2차원 텐서의 차원의 차원 1 결합 -> 2,9\n",
    "print(t16.size())  # >>> torch.Size([2, 9])\n",
    "print(t16)\n",
    "# >>> tensor([[ 0,  1,  2,  6,  7,  8, 12, 13, 14],\n",
    "#             [ 3,  4,  5,  9, 10, 11, 15, 16, 17]])\n",
    "\n",
    "print(\"#\" * 50, 4)\n",
    "\n",
    "t17 = torch.arange(0, 6).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "t18 = torch.arange(6, 12).reshape(1, 2, 3)  # torch.Size([1, 2, 3])\n",
    "\n",
    "t19 = torch.cat((t17, t18), dim=0) #3차원 텐서의 차원의 차원 0 결합 -> 2,2,3\n",
    "print(t19.size())  # >>> torch.Size([2, 2, 3])\n",
    "print(t19)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5]],\n",
    "#             [[ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t20 = torch.cat((t17, t18), dim=1) #3차원 텐서의 차원의 차원 1 결합 -> 1,4,3\n",
    "print(t20.size())  # >>> torch.Size([1, 4, 3])\n",
    "print(t20)\n",
    "# >>> tensor([[[ 0,  1,  2],\n",
    "#              [ 3,  4,  5],\n",
    "#              [ 6,  7,  8],\n",
    "#              [ 9, 10, 11]]])\n",
    "\n",
    "t21 = torch.cat((t17, t18), dim=2) #3차원 텐서의 차원의 차원 2 결합  -> 1,2,6\n",
    "print(t21.size())  # >>> torch.Size([1, 2, 6])\n",
    "print(t21)\n",
    "# >>> tensor([[[ 0,  1,  2,  6,  7,  8],\n",
    "#              [ 3,  4,  5,  9, 10, 11]]])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:56.448887200Z",
     "start_time": "2023-09-19T09:39:56.404876700Z"
    }
   },
   "id": "a85062e51ccf31c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "m_tensor_stackin.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "69729cce81b55ba6"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]]])\n",
      "torch.Size([2, 2, 3]) True\n",
      "t5 tensor([[[ 1,  2,  3],\n",
      "         [ 7,  8,  9]],\n",
      "\n",
      "        [[ 4,  5,  6],\n",
      "         [10, 11, 12]]])\n",
      "torch.Size([2, 2, 3]) True\n",
      "torch.Size([2, 3, 2]) True\n",
      "t7 tensor([[[ 1,  7],\n",
      "         [ 2,  8],\n",
      "         [ 3,  9]],\n",
      "\n",
      "        [[ 4, 10],\n",
      "         [ 5, 11],\n",
      "         [ 6, 12]]])\n",
      "################################################## 1\n",
      "torch.Size([3]) torch.Size([3])\n",
      "torch.Size([2, 3])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "True\n",
      "torch.Size([3, 2])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([[1, 2, 3], [4, 5, 6]]) #2,3 텐서 생성\n",
    "t2 = torch.tensor([[7, 8, 9], [10, 11, 12]]) #2,3 텐서 생성\n",
    "\n",
    "t3 = torch.stack([t1, t2], dim=0) # 두개를 이어 붙이고 바깥에 차원 생성\n",
    "print(t3)\n",
    "t4 = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0) # 각 텐서에 바깥차원 생성 후 cat 함수로 붙임\n",
    "print(t3.shape, t3.equal(t4)) #  torch.stack([t1, t2], dim=0) = torch.cat([t1.unsqueeze(dim=0), t2.unsqueeze(dim=0)], dim=0)\n",
    "\n",
    "t5 = torch.stack([t1, t2], dim=1) # 두개를 붙이고 1차원에 차원생성\n",
    "t6 = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1) # 각 텐서의 1차원에 차원 생성 후 cat 함수로 붙임\n",
    "print('t5',t5)\n",
    "print(t5.shape, t5.equal(t6)) # torch.stack([t1, t2], dim=1) = torch.cat([t1.unsqueeze(dim=1), t2.unsqueeze(dim=1)], dim=1)\n",
    "\n",
    "t7 = torch.stack([t1, t2], dim=2) # 2차원 원소끼리 붙임\n",
    "t8 = torch.cat([t1.unsqueeze(dim=2), t2.unsqueeze(dim=2)], dim=2) # 2차원 원소에 unsqueeze 후 cat\n",
    "print(t7.shape, t7.equal(t8))\n",
    "print('t7',t7)\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t9 = torch.arange(0, 3)  # tensor([0, 1, 2])\n",
    "t10 = torch.arange(3, 6)  # tensor([3, 4, 5])\n",
    "\n",
    "print(t9.size(), t10.size())\n",
    "# >>> torch.Size([3]) torch.Size([3])\n",
    "\n",
    "t11 = torch.stack((t9, t10), dim=0)\n",
    "print(t11.size())  # >>> torch.Size([2,3])\n",
    "print(t11)\n",
    "# >>> tensor([[0, 1, 2],\n",
    "#             [3, 4, 5]])\n",
    "\n",
    "t12 = torch.cat((t9.unsqueeze(0), t10.unsqueeze(0)), dim=0)\n",
    "print(t11.equal(t12))\n",
    "# >>> True\n",
    "\n",
    "t13 = torch.stack((t9, t10), dim=1)\n",
    "print(t13.size())  # >>> torch.Size([3,2])\n",
    "print(t13)\n",
    "# >>> tensor([[0, 3],\n",
    "#             [1, 4],\n",
    "#             [2, 5]])\n",
    "t14 = torch.cat((t9.unsqueeze(1), t10.unsqueeze(1)), dim=1)\n",
    "print(t13.equal(t14))\n",
    "# >>> True\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:57.405101900Z",
     "start_time": "2023-09-19T09:39:57.371094400Z"
    }
   },
   "id": "376c5eaf82760f14"
  },
  {
   "cell_type": "markdown",
   "source": [
    "n_tensor_vstack_hstack.py\n",
    "="
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ac4ee1054c02183"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([4, 2, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12]],\n",
      "\n",
      "        [[13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[19, 20, 21],\n",
      "         [22, 23, 24]]])\n",
      "################################################## 1\n",
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "tensor([[1, 4],\n",
      "        [2, 5],\n",
      "        [3, 6]])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 2, 3])\n",
      "torch.Size([2, 4, 3])\n",
      "tensor([[[ 1,  2,  3],\n",
      "         [ 4,  5,  6],\n",
      "         [13, 14, 15],\n",
      "         [16, 17, 18]],\n",
      "\n",
      "        [[ 7,  8,  9],\n",
      "         [10, 11, 12],\n",
      "         [19, 20, 21],\n",
      "         [22, 23, 24]]])\n"
     ]
    },
    {
     "data": {
      "text/plain": "'\\nvstack : 행으로 붙임\\nhstack : 열로 붙임\\n'"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "t1 = torch.tensor([1, 2, 3]) # 3, 텐서 생성\n",
    "t2 = torch.tensor([4, 5, 6]) # 3, 텐서 생성\n",
    "t3 = torch.vstack((t1, t2)) # 2,3 텐서\n",
    "print(t3)\n",
    "# >>> tensor([[1, 2, 3],\n",
    "#             [4, 5, 6]])\n",
    "\n",
    "t4 = torch.tensor([[1], [2], [3]]) # 3,1 텐서 생성\n",
    "t5 = torch.tensor([[4], [5], [6]]) # 3,1 텐서 생성\n",
    "t6 = torch.vstack((t4, t5)) # 6,1 텐서\n",
    "# >>> tensor([[1],\n",
    "#             [2],\n",
    "#             [3],\n",
    "#             [4],\n",
    "#             [5],\n",
    "#             [6]])\n",
    "\n",
    "t7 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "]) # 2,2,3 텐서 생성\n",
    "print(t7.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t8 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "]) # 2,2,3 텐서 생성\n",
    "print(t8.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t9 = torch.vstack([t7, t8]) # 4,2,3 텐서 생성\n",
    "print(t9.shape)\n",
    "# >>> (4, 2, 3)\n",
    "\n",
    "print(t9)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12]],\n",
    "#             [[13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\n",
    "print(\"#\" * 50, 1)\n",
    "\n",
    "t10 = torch.tensor([1, 2, 3])\n",
    "t11 = torch.tensor([4, 5, 6])\n",
    "t12 = torch.hstack((t10, t11))\n",
    "print(t12)\n",
    "# >>> tensor([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "t13 = torch.tensor([[1], [2], [3]])\n",
    "t14 = torch.tensor([[4], [5], [6]])\n",
    "t15 = torch.hstack((t13, t14))\n",
    "print(t15)\n",
    "# >>> tensor([[1, 4],\n",
    "#             [2, 5],\n",
    "#             [3, 6]])\n",
    "\n",
    "t16 = torch.tensor([\n",
    "  [[1, 2, 3], [4, 5, 6]],\n",
    "  [[7, 8, 9], [10, 11, 12]]\n",
    "])\n",
    "print(t16.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t17 = torch.tensor([\n",
    "  [[13, 14, 15], [16, 17, 18]],\n",
    "  [[19, 20, 21], [22, 23, 24]]\n",
    "])\n",
    "print(t17.shape)\n",
    "# >>> (2, 2, 3)\n",
    "\n",
    "t18 = torch.hstack([t16, t17])\n",
    "print(t18.shape)\n",
    "# >>> (2, 4, 3)\n",
    "\n",
    "print(t18)\n",
    "# >>> tensor([[[ 1,  2,  3],\n",
    "#              [ 4,  5,  6],\n",
    "#              [13, 14, 15],\n",
    "#              [16, 17, 18]],\n",
    "#             [[ 7,  8,  9],\n",
    "#              [10, 11, 12],\n",
    "#              [19, 20, 21],\n",
    "#              [22, 23, 24]]])\n",
    "\"\"\"\n",
    "vstack : 행으로 붙임\n",
    "hstack : 열로 붙임\n",
    "\"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T09:39:58.349317100Z",
     "start_time": "2023-09-19T09:39:58.302306800Z"
    }
   },
   "id": "44a6a9a5a5e70a7a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 숙제 후기\n",
    "평소에 numpy를 자주 사용하곤 하는데 사실상 gpt에게 numpy를 사용해서 코드를 작성해달라고 하지 직접 numpy에 내장된 함수들을 써본적이 거의 없었다.\n",
    "이번 과제를 통해 tensor에 대해 알게되었는데 numpy에서 자주 보았던 메서드가 보여 어떤 기능을 하는지 대충 예상할 수 있었고 numpy와 크게 다르지 않은 구조를 가지고 있는 것 같아 tensor에 대해 공부함에 거리낌 없이 진도를 나갈 수 있었다.\n",
    "\n",
    "------\n",
    "# 새롭게 알게된 것\n",
    "view와 reshape부분을 나가면서 contigugous 속성에 대해 새롭게 알게되었다.\n",
    "항상 리스트나 행렬같은 경우 행방향 순서로 메모리에 저장한다고 생각했지만 contigugous속성에 대해 알게되면서\n",
    "꼭 행방향이 아니라 열방향 순서 메모리 저장도 가능하다는 것을 알게되었다.\n",
    "\n",
    "그렇다면 왜 이부분이 중요한지 궁금했다\n",
    "contigugous 텐서는 텐서의 원소들이 메모리에 연속적으로 저장되어있는 상태를 말한다.\n",
    "연속적인 메모리 레이아웃은 데이터를 효율적으로 처리하고 다른 연산에 사용하기 쉽게 만들어준다.\n",
    "\n",
    "Non-contigugous 텐서는 원소들이 메모리에 연속적으로 저장되어 있지 않은 텐서를 말한다.\n",
    "\n",
    "contigugous와 Non-contigugous 중 contigugous 텐서를 유지하는 것이 좋은데 그 이유는 아래와 같다.\n",
    "1. 연산속도 향상 - 연속적인 메모리 레이아웃으로 데이터에 대한 접근이 빠르고 효율적이다.\n",
    "2. 하드웨어 최적화 - CPU와 GPU는 연속적인 메모리 액세스 패턴을 잘 처리한다.\n",
    "3. 메모리 효율성 - 데이터를 메모리에 연속적으로 저장하므로 메모리 사용 효율성이 높아진다.\n",
    "4. API호환성 - 많은 딥러닝 라이브러리와 프레임워크는 연속적인 텐서를 가정하고 설계가 되어있다.\n",
    "5. 메모리 복사방지 - Non-contigugous 텐서는 연산시 메모리 복사가 필요할 수 있다.\n",
    "따라서 contigugous한 텐서를 유지하는 것이 딥러닝 모델의 효율성, 성능, 및 안정성을 향상시키는데 중요하다.\n",
    "\n",
    "특정 연산 후 텐서가 contigugous하지 않을 수 있는데 이 경우에 필요에 따라 .contigugous() 메서드나 관련 함수를 사용하여 텐서를 contigugous하게 변환할 수 있다고 한다.\n",
    "\n",
    "이번 과제를 진행하면서 Non-contigugous한 텐서로 인해 오류가 발생한 적이 없지만 텐서의 속성에 대해 이해하게 되면서 앞으로 텐서를 조작하면서 고려해야할 사항을 알 수 있게되었다.\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "92f98757624737b2"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-19T10:15:58.062823100Z",
     "start_time": "2023-09-19T10:15:58.036817800Z"
    }
   },
   "id": "ac5a2dfc2f1d3f76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "3c01425e9e2720"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
